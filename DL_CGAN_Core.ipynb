{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlU3SQQzSeE4cmL206pqJf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doldol330/DL_Project/blob/main/DL_CGAN_Core.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAb9oHNgiPDm"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import xrange\n",
        "from sklearn.utils import shuffle\n",
        "from musegan.libs.ops import *\n",
        "from musegan.libs.utils import *\n",
        "from musegan.eval.metrics import *\n",
        "from musegan.components import *\n",
        "from config import *\n",
        "# from config.default_128_r_off_y_off import *\n",
        "\n",
        "###########################################################################\n",
        "# GAN\n",
        "###########################################################################\n",
        "\n",
        "class GAN(object):\n",
        "    def __init__(self, sess, config, model):\n",
        "        print ('{:=^120}'.format(' Building MidiNet '))\n",
        "\n",
        "        self.config = config\n",
        "        self.sess = sess\n",
        "\n",
        "        # create global step variable and increment op\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        self.global_step_increment = tf.assign(self.global_step, self.global_step+1)\n",
        "\n",
        "        # create generator (G)\n",
        "        self.model = model\n",
        "\n",
        "        self.summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n",
        "        self.summary = tf.summary.merge(self.summaries)\n",
        "        self.summary_image = tf.summary.merge([s for s in self.summaries if '/prediction/' in s.name])\n",
        "\n",
        "        self.model.get_model_info(quiet=False)\n",
        "\n",
        "        \"\"\" Saver \"\"\"\n",
        "        self.saver = tf.train.Saver()\n",
        "        self.saver_g = tf.train.Saver(self.model.g_vars, max_to_keep=30)\n",
        "        self.saver_d = tf.train.Saver(self.model.d_vars, max_to_keep=30)\n",
        "        self.saver_dict = {'midinet': self.saver, 'G':self.saver_g, 'D':self.saver_d}\n",
        "        print( '{:=^120}'.format('Done!'))\n",
        "\n",
        "        print('*initializing variables...')\n",
        "\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        self.dir_ckpt = os.path.join(self.config.exp_name, 'checkpoint')\n",
        "        self.dir_sample = os.path.join(self.config.exp_name, 'samples')\n",
        "        self.dir_log = os.path.join(self.config.exp_name, 'logs')\n",
        "\n",
        "        if not os.path.exists(self.dir_ckpt):\n",
        "            os.makedirs(self.dir_ckpt)\n",
        "        if not os.path.exists(self.dir_sample):\n",
        "            os.makedirs(self.dir_sample)\n",
        "        if not os.path.exists(self.dir_log):\n",
        "            os.makedirs(self.dir_log)\n",
        "\n",
        "        path_src = os.path.join(self.dir_log, 'src')\n",
        "\n",
        "        if not os.path.exists(path_src):\n",
        "            os.makedirs(path_src)\n",
        "\n",
        "        for file_path in glob.glob(\"./*.py\"):\n",
        "            copyfile(file_path, os.path.join(path_src, os.path.basename(file_path)))\n",
        "\n",
        "    def train(self, input_data):\n",
        "\n",
        "        # save training data samples\n",
        "        sample_shape = get_sample_shape(self.config.sample_size)\n",
        "        imsave(input_data.get_rand_smaples(self.config.sample_size), sample_shape, path=os.path.join(self.dir_sample, 'Train(random).png'))\n",
        "\n",
        "        feed_dict_sample = input_data.gen_feed_dict()\n",
        "\n",
        "        # training\n",
        "        counter = 0\n",
        "        num_batch = input_data.get_batch_num()\n",
        "\n",
        "        for epoch in range(self.config.epoch):\n",
        "\n",
        "            print ('{:-^120}'.format(' Epoch {} Start '.format(epoch)))\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "            for batch_idx in range(num_batch):\n",
        "\n",
        "                batch_start_time = time.time()\n",
        "\n",
        "                feed_dict_batch = input_data.gen_feed_dict(idx=batch_idx)\n",
        "\n",
        "                # update D\n",
        "                num_iters_D = 100 if counter < 25 or counter % 500 == 0 else 5\n",
        "\n",
        "                for j in range(num_iters_D):\n",
        "                    self.sess.run(self.model.d_optim, feed_dict=feed_dict_batch)\n",
        "\n",
        "                # update G\n",
        "                self.sess.run(self.model.g_optim, feed_dict=feed_dict_batch)\n",
        "\n",
        "                # compute losses\n",
        "                d_loss_eval = self.model.d_loss.eval(feed_dict_batch) * -1.\n",
        "                g_loss_eval = self.model.g_loss.eval(feed_dict_batch)\n",
        "\n",
        "\n",
        "                # print and save batch info\n",
        "                if self.config.print_batch:\n",
        "                    print( '---{}--- epoch: {:2d} | batch: {:4d}/{:4d} | time: {:6.2f} '\\\n",
        "                        .format(self.config.exp_name+'_GPU_'+self.config.gpu_num, epoch,\n",
        "                        batch_idx , num_batch, time.time() - batch_start_time))\n",
        "\n",
        "                    print ('D loss: %6.2f, G loss: %6.2f' % (d_loss_eval, g_loss_eval))\n",
        "\n",
        "                if not counter % self.config.iter_to_save:\n",
        "                    self.run_sampler(feed_dict=feed_dict_sample,\n",
        "                                        sample_size=self.config.sample_size,\n",
        "                                        prefix='train_{:03d}'.format(counter),\n",
        "                                        save_info=True,\n",
        "                                        save_dir=self.dir_sample)\n",
        "\n",
        "                    self.save(self.dir_ckpt, component='GD', global_step=self.global_step)\n",
        "\n",
        "                counter += 1\n",
        "                self.sess.run(self.global_step_increment)\n",
        "\n",
        "        print ('{:=^120}'.format(' Training End '))\n",
        "\n",
        "    def save(self, checkpoint_dir, component='all', global_step=None):\n",
        "\n",
        "        if component == 'all':\n",
        "            saver_names = ['midinet', 'G', 'D', 'invG']\n",
        "        elif component == 'GD':\n",
        "            saver_names = ['midinet', 'G', 'D']\n",
        "        elif component == 'invG':\n",
        "            saver_names = ['midinet', 'invG']\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        print('*saving checkpoints...')\n",
        "        for saver_name, saver in self.saver_dict.iteritems():\n",
        "            if saver_name in saver_names:\n",
        "                if not os.path.exists(os.path.join(checkpoint_dir, saver_name)):\n",
        "                    os.makedirs(os.path.join(checkpoint_dir, saver_name))\n",
        "                saver.save(self.sess, os.path.join(checkpoint_dir, saver_name, saver_name),\n",
        "                           global_step=global_step)\n",
        "\n",
        "    def load(self, checkpoint_dir, component='all'):\n",
        "\n",
        "        if component == 'all':\n",
        "            saver_names = ['midinet'] # ['midinet', 'G', 'D', 'invG']\n",
        "        elif component == 'GD':\n",
        "            saver_names = ['G', 'D']\n",
        "\n",
        "        print('*reading checkpoints...')\n",
        "\n",
        "        for saver_name in saver_names:\n",
        "            ckpt = tf.train.get_checkpoint_state(os.path.join(checkpoint_dir, saver_name))\n",
        "            if ckpt and ckpt.model_checkpoint_path:\n",
        "                ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "                self.saver.restore(self.sess, os.path.join(checkpoint_dir, saver_name, ckpt_name))\n",
        "                print('*load chekpoints sucessfully')\n",
        "                return True\n",
        "            else:\n",
        "                print('[!] Load failed...')\n",
        "                return False\n",
        "\n",
        "    def run_sampler(self, feed_dict, sample_size, prefix='sample', save_info=True, save_dir='./'):\n",
        "\n",
        "        # run sampler\n",
        "        print ('*running sampler...')\n",
        "        samples = self.sess.run(self.model.prediction, feed_dict=feed_dict)\n",
        "        # save results to image files\n",
        "        if save_info:\n",
        "            print('*saving files...')\n",
        "\n",
        "            sample_shape = get_sample_shape(sample_size)\n",
        "            imsave(samples, size=sample_shape, path=os.path.join(save_dir, prefix+'.png'))\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def gen_test(self, input_data, gen_dir=None, batch_size=None, num_batch=1, key='test', is_eval=True, is_save=True):\n",
        "        '''\n",
        "            type_ = 'npy', 'data', 'all'\n",
        "        '''\n",
        "        batch_size = self.config.batch_size if not batch_size else batch_size\n",
        "\n",
        "        gen_dir = os.path.join(self.config.exp_name, 'gen') if gen_dir is None else gen_dir\n",
        "\n",
        "        if not os.path.exists(gen_dir):\n",
        "            os.makedirs(gen_dir)\n",
        "\n",
        "        prediction_list = []\n",
        "\n",
        "        for bidx in range(num_batch):\n",
        "            feed_dict = input_data.gen_feed_dict(idx=bidx, data_size=batch_size, key=key)\n",
        "            s = self.run_sampler(feed_dict=feed_dict,\n",
        "                                        sample_size=batch_size,\n",
        "                                        save_info=is_save,\n",
        "                                        save_dir=gen_dir)\n",
        "            prediction_list.append(s)\n",
        "\n",
        "        result = np.concatenate(prediction_list, axis=0)\n",
        "\n",
        "        if is_save:\n",
        "            np.save(os.path.join(gen_dir, 'gen.npy'), result)\n",
        "\n",
        "        return result, eval_result\n",
        "\n",
        "###########################################################################\n",
        "# MuseGAN\n",
        "###########################################################################\n",
        "\n",
        "class MuseGAN(object):\n",
        "    def __init__(self, sess, config, model):\n",
        "        print ('{:=^120}'.format(' Building MidiNet '))\n",
        "\n",
        "\n",
        "        self.config = config\n",
        "        self.sess = sess\n",
        "\n",
        "        # create global step variable and increment op\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        self.global_step_increment = tf.assign(self.global_step, self.global_step+1)\n",
        "\n",
        "        # create generator (G)\n",
        "        self.model = model\n",
        "\n",
        "        self.summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n",
        "        self.summary = tf.summary.merge(self.summaries)\n",
        "        self.summary_image = tf.summary.merge([s for s in self.summaries if '/prediction/' in s.name])\n",
        "\n",
        "        self.model.get_model_info(quiet=False)\n",
        "\n",
        "        \"\"\" Saver \"\"\"\n",
        "        self.saver = tf.train.Saver()\n",
        "        self.saver_g = tf.train.Saver(self.model.g_vars, max_to_keep=30)\n",
        "        self.saver_d = tf.train.Saver(self.model.d_vars, max_to_keep=30)\n",
        "        self.saver_dict = {'midinet': self.saver, 'G':self.saver_g, 'D':self.saver_d}\n",
        "        print( '{:=^120}'.format('Done!'))\n",
        "\n",
        "        print('*initializing variables...')\n",
        "\n",
        "\n",
        "\n",
        "        # init metrics amd loss collection\n",
        "        self.metrics = Metrics(eval_map=self.config.eval_map,\n",
        "                    inter_pair=self.config.inter_pair,\n",
        "                    drum_filter=self.config.drum_filter,\n",
        "                    scale_mask=self.config.scale_mask,\n",
        "                    track_names=self.config.track_names)\n",
        "\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        self.dir_ckpt = os.path.join(self.config.exp_name, 'checkpoint')\n",
        "        self.dir_sample = os.path.join(self.config.exp_name, 'samples')\n",
        "        self.dir_log = os.path.join(self.config.exp_name, 'logs')\n",
        "\n",
        "        if not os.path.exists(self.dir_ckpt):\n",
        "            os.makedirs(self.dir_ckpt)\n",
        "        if not os.path.exists(self.dir_sample):\n",
        "            os.makedirs(self.dir_sample)\n",
        "        if not os.path.exists(self.dir_log):\n",
        "            os.makedirs(self.dir_log)\n",
        "\n",
        "        path_src = os.path.join(self.dir_log, 'src')\n",
        "\n",
        "        if not os.path.exists(path_src):\n",
        "            os.makedirs(path_src)\n",
        "\n",
        "        for file_path in glob.glob(\"./*.py\"):\n",
        "            copyfile(file_path, os.path.join(path_src, os.path.basename(file_path)))\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, input_data):\n",
        "\n",
        "        # save training data samples\n",
        "        sample_shape = get_sample_shape(self.config.sample_size)\n",
        "        train_samples = input_data.get_rand_smaples(self.config.sample_size)\n",
        "        save_bars(train_samples, sample_shape, file_path=self.dir_sample, name='Train(random).png')\n",
        "        # save_midis(train_samples, file_path=os.path.join(self.dir_sample, 'train.mid'))\n",
        "\n",
        "        if self.config.is_eval:\n",
        "            #evaluation\n",
        "            self.metrics.eval(input_data.x['train'][:256], quiet=False)\n",
        "\n",
        "        feed_dict_sample = input_data.gen_feed_dict()\n",
        "\n",
        "        # training\n",
        "        counter = 0\n",
        "        num_batch = input_data.get_batch_num()\n",
        "\n",
        "        for epoch in range(self.config.epoch):\n",
        "\n",
        "            print ('{:-^120}'.format(' Epoch {} Start '.format(epoch)))\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "            for batch_idx in range(num_batch):\n",
        "\n",
        "                batch_start_time = time.time()\n",
        "\n",
        "                feed_dict_batch = input_data.gen_feed_dict(idx=batch_idx)\n",
        "\n",
        "                # update D\n",
        "                num_iters_D = 100 if counter < 25 or counter % 500 == 0 else 5\n",
        "\n",
        "                for j in range(num_iters_D):\n",
        "                    self.sess.run(self.model.d_optim, feed_dict=feed_dict_batch)\n",
        "\n",
        "                # update G\n",
        "                self.sess.run(self.model.g_optim, feed_dict=feed_dict_batch)\n",
        "\n",
        "                # compute losses\n",
        "                d_loss_eval = self.model.d_loss.eval(feed_dict_batch) * -1.\n",
        "                g_loss_eval = self.model.g_loss.eval(feed_dict_batch)\n",
        "\n",
        "                # evaluation\n",
        "                if self.config.is_eval:\n",
        "                    samples_binary = self.sess.run(self.model.prediction_binary, feed_dict=feed_dict_batch)\n",
        "                    print(samples_binary.shape)\n",
        "                    score_matrix, score_pair = self.metrics.eval(samples_binary, quiet=True)\n",
        "                    self.metrics.collect(score_matrix, score_pair)\n",
        "\n",
        "                # collect loss\n",
        "                self.metrics.collect_loss({'d':d_loss_eval, 'g':g_loss_eval}) # loss\n",
        "\n",
        "                # print and save batch info\n",
        "                if self.config.print_batch:\n",
        "                    print( '---{}--- epoch: {:2d} | batch: {:4d}/{:4d} | time: {:6.2f} '\\\n",
        "                        .format(self.config.exp_name+'_GPU_'+self.config.gpu_num, epoch,\n",
        "                        batch_idx , num_batch, time.time() - batch_start_time))\n",
        "\n",
        "                    print ('D loss: %6.2f, G loss: %6.2f' % (d_loss_eval, g_loss_eval))\n",
        "\n",
        "                if not counter % self.config.iter_to_save:\n",
        "                    if self.config.is_eval:\n",
        "                        self.metrics.eval(samples_binary, quiet=False)\n",
        "\n",
        "                    self.metrics.save_history(self.dir_log)\n",
        "                    self.run_sampler(feed_dict=feed_dict_sample,\n",
        "                                        sample_size=self.config.sample_size,\n",
        "                                        prefix='train_{:03d}'.format(counter),\n",
        "                                        save_info=True,\n",
        "                                        save_dir=self.dir_sample)\n",
        "\n",
        "                    self.save(self.dir_ckpt, component='GD', global_step=self.global_step)\n",
        "\n",
        "                counter += 1\n",
        "                self.sess.run(self.global_step_increment)\n",
        "\n",
        "        print ('{:=^120}'.format(' Training End '))\n",
        "\n",
        "    def save(self, checkpoint_dir, component='all', global_step=None):\n",
        "\n",
        "        if component == 'all':\n",
        "            saver_names = ['midinet', 'G', 'D', 'invG']\n",
        "        elif component == 'GD':\n",
        "            saver_names = ['midinet', 'G', 'D']\n",
        "        elif component == 'invG':\n",
        "            saver_names = ['midinet', 'invG']\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        print('*saving checkpoints...')\n",
        "        # for saver_name, saver in self.saver_dict.iteritems():\n",
        "        for saver_name in self.saver_dict.keys():\n",
        "            saver = self.saver_dict[saver_name]\n",
        "            if saver_name in saver_names:\n",
        "                if not os.path.exists(os.path.join(checkpoint_dir, saver_name)):\n",
        "                    os.makedirs(os.path.join(checkpoint_dir, saver_name))\n",
        "                saver.save(self.sess, os.path.join(checkpoint_dir, saver_name, saver_name),\n",
        "                           global_step=global_step)\n",
        "\n",
        "    def load(self, checkpoint_dir, component='all'):\n",
        "\n",
        "        if component == 'all':\n",
        "            saver_names = ['midinet'] # ['midinet', 'G', 'D', 'invG']\n",
        "        elif component == 'GD':\n",
        "            saver_names = ['G', 'D']\n",
        "\n",
        "        print('*reading checkpoints...')\n",
        "\n",
        "        for saver_name in saver_names:\n",
        "            ckpt = tf.train.get_checkpoint_state(os.path.join(checkpoint_dir, saver_name))\n",
        "            if ckpt and ckpt.model_checkpoint_path:\n",
        "                ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "                self.saver.restore(self.sess, os.path.join(checkpoint_dir, saver_name, ckpt_name))\n",
        "                print('*load chekpoints sucessfully')\n",
        "                return True\n",
        "            else:\n",
        "                print('[!] Load failed...')\n",
        "                return False\n",
        "\n",
        "    def run_sampler(self, feed_dict, sample_size, prefix='sample', save_info=True, save_dir='./',type_=0):\n",
        "\n",
        "        # run sampler\n",
        "        print ('*running sampler...')\n",
        "        samples, samples_binary, samples_chroma = self.sess.run([self.model.prediction, self.model.prediction_binary,\n",
        "                                                                self.model.prediction_chroma], feed_dict=feed_dict)\n",
        "\n",
        "        # save results to image files\n",
        "        if save_info:\n",
        "            print('*saving files...')\n",
        "            save_midis(samples_binary, file_path=os.path.join(save_dir, prefix+'.mid'))\n",
        "\n",
        "            sample_shape = get_sample_shape(sample_size)\n",
        "            save_bars(samples, size=sample_shape, file_path=save_dir, name=prefix,type_=type_)\n",
        "            save_bars(samples_binary, size=sample_shape, file_path=save_dir, name=prefix+'_binary', type_=type_)\n",
        "            save_bars(samples_chroma, size=sample_shape, file_path=save_dir, name=prefix+'_chroma', type_=type_)\n",
        "\n",
        "        return samples, samples_binary, samples_chroma\n",
        "\n",
        "    def gen_test(self, input_data, gen_dir=None, batch_size=None, num_batch=1, key='test', z=None, is_eval=True, is_save=True,type_=0):\n",
        "        '''\n",
        "            type_ = 'npy', 'data', 'all'\n",
        "        '''\n",
        "        batch_size = self.config.batch_size if not batch_size else batch_size\n",
        "\n",
        "        gen_dir = os.path.join(self.config.exp_name, 'gen') if gen_dir is None else gen_dir\n",
        "\n",
        "        if not os.path.exists(gen_dir):\n",
        "            os.makedirs(gen_dir)\n",
        "\n",
        "        prediction_list = []\n",
        "\n",
        "        for bidx in range(num_batch):\n",
        "\n",
        "            feed_dict = input_data.gen_feed_dict(idx=bidx, data_size=batch_size, key=key, z=z)\n",
        "\n",
        "            _, sb, _ = self.run_sampler(feed_dict=feed_dict,\n",
        "                                        sample_size=batch_size,\n",
        "                                        save_info=is_save,\n",
        "                                        save_dir=gen_dir,\n",
        "                                        type_=type_)\n",
        "            prediction_list.append(sb)\n",
        "\n",
        "        result = np.concatenate(prediction_list, axis=0)\n",
        "        eval_result = self.metrics.eval(result, output_type=1) if is_eval else None\n",
        "\n",
        "        if is_save:\n",
        "            np.save(os.path.join(gen_dir, 'gen.npy'), result)\n",
        "\n",
        "        return result, eval_result"
      ]
    }
  ]
}